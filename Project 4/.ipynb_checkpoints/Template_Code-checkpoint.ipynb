{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1sa_QJ55-fu"
   },
   "source": [
    "# **Project 4 - Q Learning**\n",
    "\n",
    "For this project, you will be tasked with both implementing and explaining key components of the Q-learning algorithm.\n",
    "\n",
    "All the code deliverables has to be provided within this notebook.\n",
    "\n",
    "# 1 - Packages\n",
    "Let's first import all the packages that you will need during this assignment.\n",
    "\n",
    "* \n",
    "[numpy](https://numpy.org/) - is the main package for scientific computing with Python\n",
    "*\n",
    "[matplotlib](https://matplotlib.org/) - is a plotting library\n",
    "*\n",
    "[gym](https://gym.openai.com/docs/) - Gym is a toolkit for developing and comparing reinforcement learning algorithms.\n",
    "*\n",
    "[gym.spaces](http://gym.openai.com/docs/) - Every environment comes with an action_space and an observation_space. These attributes are of type Space, and they describe the format of valid actions and observations.\n",
    "*\n",
    "[time](https://docs.python.org/3/library/time.html?highlight=time#module-time) - will be used to track how much time each computation takes\n",
    "*\n",
    "[copy](https://docs.python.org/3/library/copy.html) - A copy is sometimes needed so one can change one copy without changing the other.\n",
    "*\n",
    "[Threading](https://docs.python.org/3/library/threading.html) - This module constructs higher-level threading interfaces on top of the lower level thread module.\n",
    "*\n",
    "[Collections](https://docs.python.org/2/library/collections.html) - This module implements specialized container datatypes providing alternatives to Python’s general purpose built-in containers, dict, list, set, and tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_PLd07ie8k1"
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Authors:\n",
    "# Nathan Margaglio (nathanmargaglio@gmail.com)                                                          \n",
    "# Mihir Hemant Chauhan (mihirhem@buffalo.edu)                       \n",
    "# Qian Cheng (qcheng2@buffalo.edu)                            \n",
    "#######################################################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import gym.spaces\n",
    "import time\n",
    "import copy\n",
    "import threading\n",
    "import time\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6AsLthEre8kw"
   },
   "source": [
    "`## Basic Environment\n",
    "Here we define our grid-world environment. No need to make any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1KZhxOunpbNp"
   },
   "outputs": [],
   "source": [
    "class GridEnvironment(gym.Env):\n",
    "    metadata = { 'render.modes': ['human'] }\n",
    "    \n",
    "    def __init__(self, normalize=False, size=4):\n",
    "        self.observation_space = gym.spaces.Box(0, size, (size,))\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.max_timesteps = size*2 + 1\n",
    "        self.normalize = normalize\n",
    "        self.size = size\n",
    "\n",
    "        # Generate State Transition Table\n",
    "        self.transition_matrix = []\n",
    "        for x in range(size + 1):\n",
    "            state_x = []\n",
    "            for y in range(size + 1):\n",
    "                state_y = []\n",
    "                for a in range(4):\n",
    "                    one_hot = np.zeros(4)\n",
    "                    one_hot[a] = 1\n",
    "                    state_y.append(one_hot)\n",
    "                state_x.append(state_y)\n",
    "            self.transition_matrix.append(state_x)\n",
    "        \n",
    "    def transition_func(self, x, y, action, return_probs=False):\n",
    "        probs = self.transition_matrix[x][y][action]\n",
    "        if return_probs:\n",
    "            return probs\n",
    "        else:\n",
    "            return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "    def _get_distance(self, x, y):\n",
    "        return abs(x[0] - y[0]) + abs(x[1] - y[1])\n",
    "        \n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [self.size, self.size]\n",
    "        self.state = np.zeros((self.size + 1, self.size + 1))\n",
    "        self.state[tuple(self.agent_pos)] = 1\n",
    "        self.state[tuple(self.goal_pos)] = 0.5\n",
    "        self.prev_distance = self._get_distance(self.agent_pos, self.goal_pos)\n",
    "        return np.array(self.agent_pos)/1.\n",
    "    \n",
    "    def step(self, action):\n",
    "        action_taken = self.transition_func(self.agent_pos[0], self.agent_pos[1], action)\n",
    "        self.state = np.random.choice(self.observation_space.shape[0])\n",
    "        if action_taken == 0:\n",
    "            self.agent_pos[0] += 1\n",
    "        if action_taken == 1:\n",
    "            self.agent_pos[0] -= 1\n",
    "        if action_taken == 2:\n",
    "            self.agent_pos[1] += 1\n",
    "        if action_taken == 3:\n",
    "            self.agent_pos[1] -= 1\n",
    "          \n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.size)\n",
    "        self.state = np.zeros((self.size + 1, self.size + 1))\n",
    "        self.state[tuple(self.agent_pos)] = 1\n",
    "        self.state[tuple(self.goal_pos)] = 0.5\n",
    "        \n",
    "        current_distance = self._get_distance(self.agent_pos, self.goal_pos)\n",
    "        if current_distance < self.prev_distance:\n",
    "            reward = 1\n",
    "        elif current_distance > self.prev_distance:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = -1\n",
    "        self.prev_distance = current_distance\n",
    "        \n",
    "        self.timestep += 1\n",
    "        if self.timestep >= self.max_timesteps or current_distance == 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        info = {}\n",
    "        \n",
    "        obs = self.agent_pos\n",
    "        if self.normalize:\n",
    "            obs = obs/self.size\n",
    "        return obs, reward, done, info\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        plt.imshow(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hy2YHvlNe8lW"
   },
   "outputs": [],
   "source": [
    "env = GridEnvironment()\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bc3VOQuJF1el"
   },
   "source": [
    "## Random Agent\n",
    "This runs the environment with a random agent that just takes random actions. Neither does he learn, nor remember anything. Try to run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9XqqwQtFr8k"
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def policy(self, observation):\n",
    "        return np.random.choice(self.action_space.n)\n",
    "        \n",
    "    def step(self, observation, verbose=False):\n",
    "        return self.policy(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0S5tBBqfF-s3"
   },
   "outputs": [],
   "source": [
    "env = GridEnvironment(normalize=True)\n",
    "agent = RandomAgent(env)\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "agent.epsilon = 0\n",
    "env.render()\n",
    "plt.show()\n",
    "\n",
    "while not done:\n",
    "    action = agent.step(obs, verbose=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCKzZh7u0zFJ"
   },
   "source": [
    "## Heuristic Agent\n",
    "This runs the environment with a heuristic agent. No need to make any changes. Try to run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVGLhrTG0yQp"
   },
   "outputs": [],
   "source": [
    "class HeuristicAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def policy(self, observation):\n",
    "        # 0 - down\n",
    "        # 1 - up\n",
    "        # 2 - right\n",
    "        # 3 - left\n",
    "        if (observation[0] < 1.):\n",
    "            return 0\n",
    "        if (observation[1] < 1.):\n",
    "            return 2\n",
    "        return 0\n",
    "        \n",
    "    def step(self, observation, verbose=False):\n",
    "        if verbose:\n",
    "            print(observation)\n",
    "        return self.policy(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZGSRSzo07v-"
   },
   "outputs": [],
   "source": [
    "env = GridEnvironment(normalize=True)\n",
    "agent = HeuristicAgent(env)\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "agent.epsilon = 0\n",
    "env.render()\n",
    "plt.show()\n",
    "\n",
    "while not done:\n",
    "    action = agent.step(obs, verbose=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSjsMD6Ze8ld"
   },
   "source": [
    "## Tabular Q-Learning\n",
    "This is where you need to define policy and update Q tables.\n",
    "For policy.\n",
    "\n",
    "*  \n",
    "[np.argmax](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html) - Returns the indices of the maximum values along an axis.\n",
    "\n",
    "### Epsilon\n",
    "\n",
    "Our agent will randomly select its action at first by a certain percentage, called ‘exploration rate’ or ‘epsilon’. This is because at first, it is better for the agent to try all kinds of things before it starts to see the patterns. When it is not deciding the action randomly, the agent will predict the reward value based on the current state and pick the action that will give the highest reward. We want our agent to decrease the number of random action, as it goes, so we indroduce an exponential-decay epsilon, that eventually will allow our agent to explore the evironment. \\\\\n",
    "\n",
    "\n",
    "\n",
    "###  <font color='red'>Task 1: Implement policy function.</font>  <br>\n",
    "**Instructions:**\n",
    "- Our agent will randomly select its action at first by a certain percentage, called ‘exploration rate’ or ‘epsilon’. This is because at first, it is better for the agent to try all kinds of things before it starts to see the patterns. Select a random uniform number. If it's less than epsilon, return the random choice action space.\n",
    "- When it is not deciding the action randomly, the agent will predict the reward value based on the current state and pick the action that will give the highest reward. \n",
    "\\begin{align} \\notag\n",
    "\\pi\\left(s_{t}\\right)=\\underset{a \\in A}{\\operatorname{argmax}} Q_{\\theta}\\left(s_{t}, a\\right)\n",
    "\\end{align} \n",
    "- Return the policy\n",
    "- Please note, that the name for all the variables should start with <mark>self</mark>, thus </br> \n",
    "\n",
    "epsilon $\\rightarrow$ self.epsilon </br> \n",
    "action_space $\\rightarrow$ self.action_space\n",
    "\n",
    "###  <font color='red'>Task 2: Update Q-table</font>  <br>\n",
    "**Instructions:**\n",
    "            \\begin{align} \\notag\n",
    "            Q^{n e w}\\left(s_{t}, a_{t}\\right) \\leftarrow(1-\\alpha) \\cdot \\underbrace{Q\\left(s_{t}, a_{t}\\right)}_{\\text {old value }}+\\underbrace{\\alpha}_{\\text {learning rate }} \\cdot \\overbrace{(\\underbrace{r_{t}}_{\\text {reward }} + \\underbrace{\\gamma}_{\\text {discount factor }} \\underbrace{\\max _{a} Q\\left(s_{t+1}, a\\right)}_{a})}^{\\text {learned value }}\n",
    "            \\end{align} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l88irSuqe8lf"
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, epsilon=1.0, lr=0.1, gamma=0.9):\n",
    "        self.env = env\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "        q_table_dim = env.observation_space.shape[0] + 1\n",
    "        self.q_table = np.zeros((q_table_dim, q_table_dim, env.action_space.n))\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def policy(self, observation):\n",
    "      # Code for policy (Task 1) (30 points)\n",
    "        \n",
    "    def step(self, observation):\n",
    "      return self.policy(observation)\n",
    "        \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        state = state.astype(int)\n",
    "        next_state = next_state.astype(int)\n",
    "        # Code for updating Q Table (Task 2) (20 points)\n",
    "        \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNKDdzD3e8lj"
   },
   "source": [
    "### Training\n",
    "### Environment\n",
    "First, we initialize our environment.  The environment, loosely structured like [OpenAI's Gym Environments](https://gym.openai.com/), has three main methods: `reset`, `step` and `render`. You'll only need `reset` and `step` here.\n",
    "\n",
    "- When we call **reset**, we initialize the environment with a fresh episode. This allows us to effectively run through episodes (only needing to call reset at the beginning of an episode), but, more importantly, `reset()` returns the environment's initial state.\n",
    "\n",
    "- The **step** method accepts an action as a parameter (which, for this example, is an integer in [0, 3]), processes the action, and returns the new state, the reward for performing the action, and a boolean indicating if the run is over.\n",
    "\n",
    "### Agent\n",
    "When we initialize the agent, we must pass both a `environment` into QLearningAgent function.\n",
    "###  <font color='red'>Task 3: Implement the training algorithm</font>  <br>\n",
    "**Instructions:**\n",
    "- After initialization, pass the initial state to obs. Then check if it's already done. If done = False, you'll keep going. While it's not done, you'll need to update `state`, `action`,`reward` and `next_state`. You can get action by `step` the current state on agent. Use `copy` to record the current state. `step` the current action on environment to return the new state, the reward for performing the action, a boolean indicating if the run is over and some other information. Add the new reward on the total rewards. Use `copy` to save the new state returned by `step`. Update the `state`, `action`, `reward`, `next_state` of agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SDbl2Kue8lk"
   },
   "outputs": [],
   "source": [
    "env = GridEnvironment() # note: we do not normalize\n",
    "agent = QLearningAgent(env)\n",
    "episodes = 1000 # number of games we want the agent to play\n",
    "delta_epsilon = agent.epsilon/episodes\n",
    "\n",
    "total_rewards = []\n",
    "epsilons = [agent.epsilon]\n",
    "\n",
    "# Training Process (Task 3) (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHDJV4ZAE2l8"
   },
   "source": [
    "#### Visualize $\\epsilon$\n",
    "Plot our value of $\\epsilon$ over each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q137fw4je8ln"
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Episode')\n",
    "plt.ylabel('$\\epsilon$')\n",
    "plt.plot(epsilons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30Yr74K2e8lr"
   },
   "source": [
    "#### Visualize Rewards\n",
    "Plot total_rewards per episode.  We apply a rolling mean of window $10$ to visualize easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Raqojkywe8ls"
   },
   "outputs": [],
   "source": [
    "window = 10\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward (SMA 10)')\n",
    "plt.plot([np.mean(total_rewards[tr:tr+window]) for tr in range(window, len(total_rewards))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6wai0MeiGVG"
   },
   "outputs": [],
   "source": [
    "env = GridEnvironment()\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "agent.epsilon = 0\n",
    "env.render()\n",
    "plt.show()\n",
    "\n",
    "while not done:\n",
    "    action = agent.step(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Template of Tabular Q-Learning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
